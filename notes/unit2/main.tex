\section{Unit 2}
\subsection{Lecture 8: Matrix Operations [2.1]}

\begin{impbox}{}{}
    Before I start, I must note that I was not able to physically make it to lecture 8, so these notes are simply just a brief overview of concepts covered in lecture.
\end{impbox}

So far, we've very briefly been introduced to the idea of matrix multiplication through our studies of systems of equation in the form $A\vec{x}=\vec{b}$ which we know can be written as
\[
    \begin{pmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        \vdots & & \ddots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{pmatrix}
    \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} 
    = 
    \begin{pmatrix}
        b_1 \\ \vdots \\ b_n
    \end{pmatrix}    
\]
in order to represent the system 
\[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1 \\
        \vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_m &= b_n
    \end{cases}
\]
You'll notice that in order to go from the matrix to the system we simply take each row of the matrix (or each coefficent) and multiply it by the corresponding variable. This is exactly how matrix multiplication is defined.

\begin{defbox}{Matrix Multiplication}{}
    Consider two matricies $\underset{m \times n}{A}$ and $\underset{n \times k}{B}$ such that 
    \[
        A = \begin{pmatrix}
            a_{11} & a_{12} & \cdots & a_{1n} \\
            \vdots & & \ddots \\
            a_{m1} & a_{m2} & \cdots & a_{mn}
        \end{pmatrix} 
        \text{ and }
        B = \begin{pmatrix}
            b_{11} & b_{12} & \cdots & b_{1k} \\
            \vdots & & \ddots \\
            b_{n1} & b_{n2} & \cdots & b_{nk}
        \end{pmatrix} 
    \]
    Label the columns in $B$ as $\bf{b}_1, \bf{b}2, \dots\, \bf{b}_k$. Then their product, $AB$, is defined as follows:
    \[
        \underset{m \times k}{AB} = \begin{pmatrix}
            A\bf{b}_1 & A\bf{b}_2 & \cdots & A\bf{b}_k
        \end{pmatrix}
    \]
\end{defbox}

It's extremely important to note that matrix multiplication is only defined two matricies, $A$ and $B$, \underline{if, and only if, the number of columns in $A$ is equal to the number of rows in $B$.}

One other way to think of matrix multiplication that if $A$ and $B$ are defined as above then, 
\[
    AB = \begin{pmatrix}
        \sum_{i=1}^{n} a_{1i}b_{i1} & \sum_{i=1}^{n} a_{1i}b_{i2} & \cdots\, & \sum_{i=1}^{n} a_{mi}b_{ik} \\
        \sum_{i=1} a_{2i}b_{i1} & \sum_{i=1}^{n} a_{2i}b_{i2} & \cdots & \sum_{i=1}^{n} a_{2i}b_{ik}\\
        \vdots & & \ddots & \\ 
        \sum_{i=1}^{n} a_{mi}b_{i1} & \sum_{i=1}^{n} a_{mi}b_{i2} & \cdots & \sum_{i=1}^{n} a_{mi}b_{ik}
    \end{pmatrix}
\]
Now, granted, this looks very complicated and extremely hard to use. So, why did I bother including this here? Well, if you look closely at each summation, what you'll notice is that each sumation is simply the sum of the product of a corresponding \textit{row} from $A$ with a column from $B$. So, for example, the first element in $AB$ is the sum of each element in the \textbf{first} \textit{row} in $A$ multiplied by its corresponding element in the \textbf{first} \textit{column} in $B$. The second element in $AB$ is the \textbf{first} \textit{row} in $A$ multiplied by the \textbf{second} \textit{column} in $B$. This is probably best explained with a few examples:

\begin{example}{Basic Matrix Multiplication}{}
    Let $A$ and $B$ be defined as below. Determine if $AB$ is defined, and if it is find it.
    \[
        \underset{2 \times 2}{A} = \begin{pmatrix}
            1 & 2 \\ 3 & 4
        \end{pmatrix} 
        \text{ and }
        \underset{2 \times 2}{B} = \begin{pmatrix}
            2 & 4 \\ 1 & 3
        \end{pmatrix}
    \]
    \begin{solution}
        First, since $A$ and $B$ are both $2\times\,2$ matricies, we know that $AB$ must exists since $A$ and $B$, obviously, have the same number of rows and columns as each other. We can then find $AB$:
        \[
            AB = \begin{pmatrix}
                (1 \cdot 2) + (2 \cdot 1) & (1 \cdot 4) + (2 \cdot 3) \\
                (3 \cdot 2) + (4 \cdot 1) & (3 \cdot 4) + (4 \cdot 3)
            \end{pmatrix} 
            = 
            \begin{pmatrix}
                4 & 10 \\ 10 & 24
            \end{pmatrix}
        \]
    \end{solution}
\end{example}
Keep in mind that we also could've found the product $AB$ using the other definition of matrix multiplication:
\[
    AB = \begin{pmatrix}
        A\bf{b}_1 & A\bf{b}_2
    \end{pmatrix}
    = 
    \begin{pmatrix}
        \begin{pmatrix}
            1 & 2 \\ 3 & 4
        \end{pmatrix}
        \begin{pmatrix}
            2 \\ 1 
        \end{pmatrix}
        &
        \begin{pmatrix}
            1 & 2 \\ 3 & 4
        \end{pmatrix}
        \begin{pmatrix}
            4 \\ 3 
        \end{pmatrix}
    \end{pmatrix}
    = 
    \begin{pmatrix}
        \begin{pmatrix}
            4 \\ 10 
        \end{pmatrix}
        \begin{pmatrix}
            10 \\ 24
        \end{pmatrix}
    \end{pmatrix}
    = 
    \begin{pmatrix}
        4 & 10 \\ 10 & 24
    \end{pmatrix}
\]

\begin{example}{Basic Matrix Multiplication}{}
    Let $A$ and $B$ be defined as below. Determine if $AB$ is defined, and if it is find it.
    \[
        \underset{3 \times 2}{A} = \begin{pmatrix}
           1 & 3 \\
           5 & 7 \\
           9 & 11 \\ 
        \end{pmatrix}
        \text{ and }
        \underset{2 \times 2}{B} = \begin{pmatrix}
            1 & 1 \\ 0 & 1
        \end{pmatrix}
    \]
    \begin{solution}
        Since $A$ has $2$ columns and $B$ has $2$ rows, we know that $AB$ exists. Thus, 
        \[
            AB = \begin{pmatrix}
                \left(1 \cdot 1\right) + \left(3 \cdot 0\right) & \left(1 \cdot 1\right) + \left(3 \cdot 1\right) \\
                \left(5 \cdot 1\right) + \left(7 \cdot 0\right) & \left(5 \cdot 1\right) + \left(7 \cdot 1\right) \\
                \left(9 \cdot 1\right) + \left(11 \cdot 0\right) & \left(9 \cdot 1\right) + \left(11 \cdot 1\right)
            \end{pmatrix}
            = 
            \begin{pmatrix}
                1 & 4 \\
                5 & 12 \\
                9 & 20
            \end{pmatrix}
        \]
    \end{solution}
\end{example}

\begin{thm}{Properties of Matrix Multiplication}{}
    Consider a matrix $\underset{m\times\,n}{A}$ and let $B$ and $C$ be matricies such that the necessary products are defined.
    \begin{enumerate}
        \item Associative Law: $A(BC) = (AB)C$
        \item Left Distributive Law: $A(B+C) = AB + AC$
        \item Right Distributive Law: $(B+C)A = BA + CA$
        \item $r(AB) = (rA)B = A(rB)$ for any $r \in \RR$
        \item Identity for Matrix Multiplication: $I_{m}A = A = AI_{n}$
    \end{enumerate}
\end{thm}

Recall that $I_n$ is the $n\times\,n$ matrix such that every element is zero except for $a_{11}, a_{22}, \dots, a_{nn}$, referred to as the \textbf{Identity Matrix}. 

Its important to note a few things about the above theorem. 
\begin{enumerate}
    \item There is NO communative law, thus, you cannot assume $AB = BA$
    \item There is NO cancelation law: $AB=BC$ does NOT imply that $A=C$
    \item If $AB$ is the zero matrix, you CANNOT conclude that either $A=0$ or $B=0$ 
\end{enumerate}

\begin{defbox}{The Power of a Matrix}{}
    Let $A$ be any $n\times\,n$ matrix. Then $A^{k}$ is defined as 
    \[
        A^{k} = \underbrace{A \cdots A}_{k \text{ times}}
    \]
    If $k=0$ then $A^{k}=I_n$ where $I_n$ is the identity matrix in $\RR^{n}$. 
\end{defbox}
The definition of the power of a matrix should feel extremely similar to the definition of a real number to any power: repeated multiplication. 

\begin{defbox}{Transpose of a Matrix}{}
    Let $A$ be any $m\times\,n$ matrix. Then $A^{\text{T}}$ is the \textbf{transpose} of $A$, which is a $n \times\, m$ matrix who's \underline{rows are made from the columns of $A$ and whose columns are made from the rows}. 
\end{defbox}

For example, let $A$ and $B$ be defined as 
\[
    A = \begin{pmatrix}
        a & b \\ c & d
    \end{pmatrix}
    \text{ and }
    B = \begin{pmatrix}
        1 & 2 & 3 \\ 4 & 5 & 6
    \end{pmatrix}
\]
then, 
\[
    A^{\text{T}} = \begin{pmatrix}
        a & c \\ b & d
    \end{pmatrix}
    \text{ and }
    B^{\text{T}} = \begin{pmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{pmatrix}
\]

\begin{thm}{Properties of the Transpose}{}
    Let $A$ and $B$ be matricies such that necessary sums and products are defined. Then, 
    \begin{enumerate}
        \item ${\left(A^{\text{T}}\right)}^{\text{T}} = A$
        \item ${\left(A+B\right)}^{\text{T}} = A^{\text{T}} + B^{\text{T}}$
        \item ${\left(rA\right)}^{\text{T}} = rA^{\text{T}}$ for any $r \in \RR$
        \item ${\left(AB\right)}^{\text{T}} = B^{\text{T}}A^{\text{T}}$
    \end{enumerate}
\end{thm}

\subsection{Lecture 9: Inverse of a Matrix and Characterizations of Inveritble Matricies [2.2-2.3]}

\begin{defbox}{Inveritble Matricies}{}
    An $n \times n$, $A$, matrix is called \underline{invertible} if there exists an $n \times n$ matrix, $B$, such that $AB=BA=I_{n}$. Such $B$ is called the inverse of $A$, denoted $A^{-1}$. $A^{-1}$ is always a unique matrix for any $A$.
\end{defbox}

\begin{thm}{}{}
    An $n \times n$ matrix is inveritble if, and only if, $\rank{A}=n$
    \begin{proof}
        Consider a matrix $\underset{n\times\,n}{A}$ such that $\rank{A_\text{RREF}}=n$. Then $A_{\text{RREF}}=I_n$. Assume it takes $k$ iterations of elementary row operations (ERO) to obtain $A_{\text{RREF}}$ from $A$. Then, $A_{\text{RREF}}=I_n=E_k\cdots\,E_2E_1$. So, $I=BA \Leftrightarrow A^{-1}=B$ and $B^{-1}=A$ and $B^{-1}=E^{-1}_{1}E^{-1}_{2}\cdots\,E^{-1}_{k}$. 

        If $A^{-1}$ exists then consider $A\vec{x}=\vec{b} \Rightarrow \vec{x}=A^{-1}\vec{b}$. Thus, the system has only one solution and so $\rank{A}=n$.
    \end{proof}
\end{thm}

If you're confused on why $\vec{x}=A^{-1}\vec{b}$ implies that the system only has a single unique solution, consider that if $\vec{x}=A^{-1}\vec{b}$ then we have 
\[
    A\vec{x} = A\left(A^{-1}\vec{b}\right) = \left(AA^{-1}\right)\vec{b} = I\vec{b} = \vec{b}
\]
so, $A^{-1}\vec{b}$ must be a solution. To show that it is unique, consider any solution $\vec{u}$. Then, 
\[
    A\vec{u} = \vec{b} \Rightarrow A^{-1}A\vec{u} = A^{-1}\vec{b} \Rightarrow I\vec{u} = A^{-1}\vec{b} \Rightarrow \vec{u} = A^{-1}\vec{b}
\]
Then, we know from~\ref{lecture:3}, that if a system has a single solution, its augmented matrix (or $A_{\text{REF}}$) has a rank equal to $n$. It should follow from this that if $A^{-1}$ exists the homogenous system, $A\vec{x}=\vec{0}$ has a single solution, the trivial solution: $\vec{x}=\vec{0}$.

\begin{impbox}{How to Find the Inverse of a $2\times\,2$ Matrix}{}
    Consider $2 \times 2$ matricies $A, B$ with rank $2$ such that $AB=I_2$,
    \[
        A = \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} 
        \text{ and }
        B = \begin{pmatrix}
            \alpha & \beta \\ \gamma & \delta
        \end{pmatrix}
    \]
    Since $\rank{A}=2$, $A$ is invertible. Then,
    \[
        AB = 
        \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} 
        \begin{pmatrix}
            \alpha & \beta \\ \gamma & \delta
        \end{pmatrix}
        = \begin{pmatrix}
            1 & 0 \\ 0 & 1 
        \end{pmatrix}
        \Rightarrow
        \begin{pmatrix}
            a\alpha + b\gamma & a\beta + b\delta \\
            c\alpha + d\gamma & c\beta + d\delta 
        \end{pmatrix}
        = 
        \begin{pmatrix}
            1 & 0 \\ 0 & 1
        \end{pmatrix}
    \]
    This gives, 
    \[
        \left.\begin{cases}
            a\alpha + b\gamma &= 1 \\
            c\alpha + d\gamma &= 0
        \end{cases}\right]
        \Rightarrow
        \begin{amatrix}{2}
            a & b & 1 \\ 
            c & d & 0
        \end{amatrix}
        \xrightarrow{\text{ERO}}
        \begin{amatrix}{2}
            1 & 0 & \alpha \\
            0 & 1 & \gamma
        \end{amatrix}_{\text{RREF}}
    \]
    \[
        \left.\begin{cases}
            a\beta + b\delta &= 0 \\
            c\beta + d\delta &= 1 
        \end{cases}\right]
        \Rightarrow
        \begin{amatrix}{2}
            a & b & 0 \\
            c & d & 1
        \end{amatrix}
        \xrightarrow{\text{ERO}}
        \begin{amatrix}{2}
           1 & 0 & \beta \\
           0 & 1 & \delta 
        \end{amatrix}_{\text{RREF}}
    \]
    We can then combine to form one large matrix
    \[
        \begin{amatrix}{1}
            A & I
        \end{amatrix} 
        \xrightarrow{\text{ERO}}
        \begin{amatrix}{1}
            I & A^{-1}
        \end{amatrix}
        \Rightarrow
        \begin{Lamatrix}{4}{2}
            1 & 0 & \alpha & \beta \\
            0 & 1 & \gamma & \delta
        \end{Lamatrix}
    \]
\end{impbox}

Note that this method can actually be generalized such that for any matrix $\underset{n\times\,n}{A}$, augmenting it with the identity matrix and using ERO to find $(A\,|\,I)_{\text{RREF}}$ will always result in $(I\,|\,A^{-1})$

\begin{example}{}{}
    Find the inverse of $A$ and verify that $AA^{-1}=I$
    \[A = \begin{pmatrix}
        1 & 3 \\ 2 & 4
    \end{pmatrix}\]
    \begin{solution}
        \begin{align*}
            \begin{Lamatrix}{4}{2}
                1 & 3 & 1 & 0 \\
                2 & 4 & 0 & 1
            \end{Lamatrix}
            &\xrightarrow{R_2 \to R_2 - 2R_1}
            \begin{Lamatrix}{4}{2}
                1 & 3 & 1 & 0 \\
                0 & -2 & -2 & 1
            \end{Lamatrix}
            \xrightarrow{R_2 \to -\frac{1}{2}R_2}
            \begin{Lamatrix}{4}{2}
                1 & 3 & 1 & 0 \\
                0 & 1 & 1 & -\frac{1}{2}
            \end{Lamatrix} \\
            &\xrightarrow{R_1 \to R_1 - 3R_2}
            \begin{Lamatrix}{4}{2}
                1 & 0 & -2 & \frac{3}{2} \\
                0 & 1 & 1 & -\frac{1}{2}
            \end{Lamatrix} \\
            &\Rightarrow A^{-1} = \begin{pmatrix}
                -2 & \frac{3}{2} \\
                1 & -\frac{1}{2}
            \end{pmatrix}
            = 
            -\frac{1}{2}\begin{pmatrix}
                4 & -3 \\
                2 & 1
            \end{pmatrix}
        \end{align*}
        Then, 
        \[
            AA^{-1} = 
            -\frac{1}{2}\begin{pmatrix}
                1 & 3 \\ 2 & 4 
            \end{pmatrix} 
            \begin{pmatrix}
                4 & -3 \\ -2 & 1
            \end{pmatrix}
            = 
            -\frac{1}{2}\begin{pmatrix}
                -2 & 0 \\
                0 & -2
            \end{pmatrix}
            = 
            \begin{pmatrix}
                1 & 0 \\ 0 & 1
            \end{pmatrix}
            = 
            I_2
        \]
    \end{solution}
\end{example}

Now, you might have noticed that in the previous example, $A^{-1}$ actaully looked extremely similar to $A$. In fact, $A$ and $A^{-1}$ are made up elements of the exact same magnitude. This phenomenon will occur for the inverses of every $2\times\,2$ matrix with rank $2$ and, in fact, we have a formula that explains the similarity. 

\begin{impbox}{Formula for Finding the Inverse of a $2 \times 2$ matrix}{}
    Let $A$ be a $2 \times 2$ matrix, 
    \[A = \begin{pmatrix}
        a & b \\ c & d
    \end{pmatrix}\]
    Then, 
    \[
        A^{-1} = \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix}^{-1}
        = 
        \frac{1}{ad - bc}\begin{pmatrix}
            d & -b \\
            -c & a
        \end{pmatrix}
    \]
    where the quanity $ad-bc$ is called the \textbf{determinant of $A$}, known as $\det{A}$.
\end{impbox}
It should follow from the last line that $\det{A} \neq 0 \Leftrightarrow A^{-1}$ exists.

\begin{example}{}{}
    For $A$ below, does $A^{-1}$ exist? If so, find $A^{-1}$
    \[A = \begin{pmatrix}
        -4 & 6 \\ 6 & -9
    \end{pmatrix}\] 
    \begin{solution}
        Start by finding $\det{A}$: 
        \[
            \det{A} = \left(-4 \cdot -9\right) - \left(4 \cdot 9\right) = 36 - 36 = 0
        \]
        Thus, by the fact above, $A^{-1}$ must not exist. Or, in other words, $A$ is \underline{not} invertible.
    \end{solution}
\end{example}

\begin{example}{}{}
    For $A$ below, does $A^{-1}$ exist? If so, find $A^{-1}$.
    \[
        A = \begin{pmatrix}
            1 & -5 & -4 \\
            0 & 3 & 4 \\
            -3 & 6 & 0
        \end{pmatrix} 
    \]
    \begin{solution}
        Start by finding the rank of $A$: 
        \[
            A \xrightarrow{R_3 \to R_3 + 3R_1}
            \begin{pmatrix}
                1 & -5 & -4 \\
                0 & 3 & 4 \\
                0 & -9 & -12
            \end{pmatrix} 
            \xrightarrow{R_3 \to R_3 + 3R_2}
            \begin{pmatrix}
                1 & -5 & -4 \\
                0 & 3 & 4 \\
                0 & 0 & 0
            \end{pmatrix}
        \]
        Therefore, $\rank{A} < 3$ so $A$ is \underline{not} invertible.
    \end{solution}
\end{example}

\begin{example}{}{}
    For $A$ below, does $A^{-1}$ exist? If so, find $A^{-1}$.
    \[
        A = \begin{pmatrix}
            1 & -5 & -4 \\
            0 & 3 & 5 \\
            -3 & 6 & 0
        \end{pmatrix} 
    \]
    \begin{solution}
        Start by finding the rank of $A$: 
        \[
            A \xrightarrow{R_3 \to R_3 + 3R_1}
            \begin{pmatrix}
                1 & -5 & -4 \\
                0 & 3 & 5 \\
                0 & -9 & -12
            \end{pmatrix} 
            \xrightarrow{R_3 \to R_3 + 3R_2}
            \begin{pmatrix}
                1 & -5 & -4 \\
                0 & 3 & 4 \\
                0 & 0 & 3
            \end{pmatrix}
        \]
        Therefore, $\rank{A} = 3$ so $A$ \underline{is} invertible. Now, consider 
        \begin{align*}
            \begin{amatrix}{1} A & I_3 \end{amatrix} 
            = 
            &\begin{Lamatrix}{9}{3}
                1 & -5 & -4 & 1 & 0 & 0 \\
                0 & 3 & 5 & 0 & 1 & 0 \\
                -3 & 6 & 0 & 0 & 0 & 1
            \end{Lamatrix}
            \xrightarrow{R_3 \to R_3 + 3R_1}
            \begin{Lamatrix}{9}{3}
                1 & -5 & -4 & 1 & 0 & 0 \\
                0 & 3 & 5 & 0 & 1 & 0 \\
                0 & -9 & -12 & 3 & 0 & 1
            \end{Lamatrix} \\
            \xrightarrow{R_3 \to R_3 + 3R_2}
            &\begin{Lamatrix}{9}{3}
                1 & -5 & -4 & 1 & 0 & 0 \\
                0 & 3 & 5 & 0 & 1 & 0 \\
                0 & 0 & 3 & 3 & 3 & 1
            \end{Lamatrix}
            \xrightarrow[R_3 \to \frac{1}{3}R_3]{R_2 \to \frac{1}{3}R_2}
            \begin{Lamatrix}{9}{3}
                1 & -5 & -4 & 1 & 0 & 0 \\
                0 & 1 & \frac{5}{3} & 0 & \frac{1}{3} & 0 \\
                0 & 0 & 1 & 1 & 1 & \frac{1}{3}
            \end{Lamatrix} \\
            \xrightarrow[R_1 \to R_1 + 4R_3]{R_2\to\,R_2 - \frac{5}{3}R_3}
            &\begin{Lamatrix}{9}{3}
                1 & -5 & 0 & 5 & 4 & \frac{4}{3} \\
                0 & 1 & 0 & -\frac{5}{3} & -\frac{4}{3} & -\frac{5}{9} \\
                0 & 0 & 1 & 1 & 1 & \frac{1}{3}
            \end{Lamatrix}
            \xrightarrow{R_1 \to R_1 + 5R_2}
            \begin{Lamatrix}{9}{3}
                1 & 0 & 0 & -\frac{10}{3} & -\frac{8}{3} & -\frac{13}{9} \\
                0 & 1 & 0 & -\frac{5}{3} & -\frac{4}{3} & -\frac{5}{9} \\
                0 & 0 & 1 & 1 & 1 & \frac{1}{3}
            \end{Lamatrix} \\
            &\Rightarrow \boxed{A^{-1} = \begin{pmatrix}
                -\frac{10}{3} & -\frac{8}{3} & -\frac{13}{3} \\ 
                -\frac{5}{3} & -\frac{4}{3} & -\frac{5}{9} \\
                1 & 1 & 3
            \end{pmatrix}}
        \end{align*}
    \end{solution}
\end{example}

\begin{example}{Is $A$ invertible?}{}
    Let $A$ be the matrix below. Determine if $A$ is inveritble.
    \[A = \begin{pmatrix}
        -7 & 0 & 4 \\
        3 & 0 & -1 \\
        2 & 0 & 9
    \end{pmatrix}\] 
    \begin{solution}
        Since $A$ has a $0$ column, $\rank{A} < 3 \therefore A$ is \underline{not} invertible
    \end{solution}
\end{example}

\begin{example}{Is $A$ invertible?}{}
    Let $A$ be the matrix below. Determine if $A$ is invertible.
    \[
        A = \begin{pmatrix}
            -1 & -3 & 0 & 1 \\
            3 & 5 & 8 & -3 \\
            -2 & -6 & 3 & 2 \\
            0 & -1 & 2 & 1 
        \end{pmatrix} 
    \]
    \begin{solution}
        \[
            A \xrightarrow[\substack{R_3 \to R_3 - 2R_1 \\ R_2 \to -\frac{1}{4}R_2}]{R_2 \to R_2 + 3R_1}
            \begin{pmatrix}
                -1 & -3 & 0 & 1 \\
                0 & 1 & -2 & 0 \\
                0 & 0 & 3 & 0 \\
                0 & -1 & 2 & 1 
            \end{pmatrix}
            \xrightarrow{R_4 \to R_4 + R_2}
            \begin{pmatrix}
                -1 & -3 & 0 & 1 \\
                0 & 1 & -2 & 0 \\
                0 & 0 & 3 & 0 \\
                0 & 0 & 0 & 1 
            \end{pmatrix}
        \]
        Thus, $\rank{A}=4 \therefore A$ \underline{is} invertible.
    \end{solution}
\end{example}

\subsection{Lecture 10: Review for Midterm 1}
\subsubsection*{Systems of Equations}
\textit{Homogenous} systems of equations are systems in the form $A\vec{x}=\vec{0}$. These systems always contain at least one solution, $\vec{x} = \vec{0}$ (known as the trivial solution). \textit{Non-Homogenous} systems of equations are system in the form $A\vec{x}=\vec{b}$ where $\vec{b} \ne \vec{0}$.

\begin{example}{Solve the System}{}
    \[\begin{cases}
        2x_1 + x_2 - x_3 &= 4 \\
        x_1 - 2x_2 + x_3 &= 2
    \end{cases}\]
    \begin{solution}
        We can form an augmented matrix and use elementary row operations to find a solution:
        \[
            \begin{Lamatrix}{4}{3}
                2 & 1 & -1 & 4 \\
                1 & -2 & 1 & 2 
            \end{Lamatrix} 
            \xrightarrow{R_1 \leftrightarrow R_2}
            \begin{Lamatrix}{4}{3}
                1 & -2 & 1 & 2 \\
                2 & 1 & -1 & 4 
            \end{Lamatrix} 
            \xrightarrow{R_2 \to R_2 - 2R_1}
            \begin{Lamatrix}{4}{3}
                1 & -2 & 1 & 2 \\
                0 & 5 & -3 & 0 
            \end{Lamatrix} 
        \]
        Now, because there is no pivot in the third column, $x_3$ is a free variable: let $x_3 = 5t$ for $t \in \RR$. Then, we can back substitute from the matrix to the equations to get: 
        \[
            \left.\begin{cases}
                5x_2 - 3x_3 &= 0 \\
                x_1 - 2x_2 + x_3 &= 2
            \end{cases}\right]
            \Rightarrow 
            x_2 = 3t \text{ and } x_1 = 2+t
        \]
        We can formally write our solution as follows:
        \[
            X_{\text{gen}} = \begin{pmatrix}
                2 + t \\ 3t \\ 5t
            \end{pmatrix}
            = 
            t\underbrace{\begin{pmatrix}
                1 \\ 3 \\ 5
            \end{pmatrix}}_{X_{\text{genhom}}}
            + \overbrace{\begin{pmatrix}
                2 \\ 0 \\ 0
            \end{pmatrix}}^{X_p}
        \]
        Where $X_{\text{genhom}}$ is the general homogenous solution and $X_p$ is a particular solution. 
    \end{solution}
\end{example}

A system is \textbf{consistent} if there exists at least one solution. $A\vec{x}=\vec{b}$ is consisten if the augmented matrix, $\left(A\,|\,B\right)_{\text{REF}}$ has no pivots in the final column. Equivalently: if $\rank{\underset{m\times\,n}{A}}=m$ then $A\vec{x}=\vec{b}$ is always consistent. We can note a few things from this fact. If $A\vec{x}=\vec{b}$ is consistent, then the span of the columns of $A$ is $\RR^{m}$. As well, consider a linear transformation $\mathcal{T}:\RR^{n}\to\,\RR^{m}$ given by the map $\vec{x} \mapsto A\vec{x}$, then $\mathcal{T}$ will be onto. If the system $A\vec{x}=\vec{b}$ has only a single solution, then $\rank{\underset{m\times\,n}{A}} = n$. Again, consider a linear transformation, $\mathcal{T}:\RR^{m}\to\,\RR^{n}$ given by the mapping $\vec{x}\mapsto\,A\vec{x}$, then $\mathcal{T}$ will be one-to-one. If a linear transformation is one-to-one \textbf{and} onto, then it is called invertible.

\begin{impbox}{}{}
    Recall that $\rank{\underset{m\times\,n}{A}} \leq \min\braces{m, n}$, so in order for a linear transformation to be both one-to-one and onto, $m$ must equal $n$ (in other words: $A$ must be a square matrix).
\end{impbox}

\subsubsection*{Span/Linear Dependence}
Consider a set of vectors, $\braces{\vec{u_1}, \vec{u_2} \dots, \vec{u_k}}$. Then, the \textbf{span} of that set of vectors, denoted $\Span{\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}}$, is the set of all possible linear combinations of $\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}$: 
\[
    c_1\vec{u_1} + c_2\vec{u_2} + \cdots + c_k\vec{c_k} = AC, \hspace{0.2cm} 
    A = \begin{pmatrix}
        \left(\vec{u_1}\right) & \left(\vec{u_2}\right) & \cdots & \left(\vec{u_k}\right) 
    \end{pmatrix}
    \text{ and }
    C = \begin{pmatrix}
        c_1 \\ c_2 \\ \vdots \\ c_k
    \end{pmatrix}
\]
The set $\braces{\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}}$ is called the \textbf{generating set}.

Consider $AC = \vec{0}$. If the only solution to $AC=\vec{0}$ is the trivial solution, then the vectors $\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k}$ are called \textbf{linearly independent}. If there exists a non-trivial solution to $AC=\vec{0}$, then the vectors are called \textbf{linearly dependent}. If the vectors are linearly dependent, then at least on vector in the generating set is redudant. We can determine which vector is redudant by finding $A_{\text{REF}}$ and noting that any columns without a pivot correspond to redudant vectors. Notice that we can easily determine if vectors are linearly independent or dependent simply by just finding $A_{\text{REF}}$. If $\rank{A_{\text{REF}}} < n$ then the columns will \underline{always be linearly dependent}. If $\rank{A_\text{REF}}=n$, then the columns will \underline{always be linearly independent}.

\begin{example}{}{}
    Determine a value of $r$ such that the set below is linearly dependent. 
    \[
        \braces{\begin{pmatrix}
            1 \\ -1 \\ 3
        \end{pmatrix}, 
        \begin{pmatrix}
            2 \\ 4 \\ -2
        \end{pmatrix}, 
        \begin{pmatrix}
            r \\ 1 \\ 2
        \end{pmatrix}}
    \]
    \begin{solution}
        Consider 
        \[
            A = \begin{pmatrix}
                1 & 2 & r \\
                -1 & 4 & 1 \\
                3 & -2 & 2
            \end{pmatrix} 
            \xrightarrow[R_3 \to R_3 - 3R_1]{R_2 \to R_2 + R1}
            \begin{pmatrix}
                1 & 2 & r \\
                0 & 6 & 1 + r \\
                0 & -8 & 2 - 3r
            \end{pmatrix}
            \xrightarrow{R_3 \to R_3 + \frac{8}{6}R_2}
            \begin{pmatrix}
                1 & 2 & r \\
                0 & 6 & 1 + r \\
                0 & 0 & \frac{10}{3} - \frac{5}{3}r
            \end{pmatrix}
            = A_{\text{REF}}
        \]
        Therefore, in order for $\rank{A_\text{REF}}$ to be less then $n=3$, $r$ must be equal to $2$. So, $\boxed{r=2}$
    \end{solution}
\end{example}

\subsubsection*{Inverses}
A $n\times\,n$ matrix is invertible if, and only if, it has rank $n$.

\begin{example}{}{}
    Let $A$ be the matrix below. Find $A^{-1}$ 
    \[
        A = \begin{pmatrix}
            1 & -2 & 3 \\
            2 & 1 & 1 \\
            -1 & 0 & 2
        \end{pmatrix} 
    \]
    \begin{solution}
        We can find $A^{-1}$ by augmenting $A$ with $I$ and finding $\left(A\,|\,I\right)_\text{RREF}=\left(I\,|\,A^{-1}\right)$.
        \begin{align*}
            \left(A\,|\,I\right) = &\begin{Lamatrix}{9}{3}
                1 & -2 & 3 & 1 & 0 & 0 \\
                2 & 1 & 1 & 0 & 1 & 0 \\
                -1 & 0 & 2 & 0 & 0 & 1 
            \end{Lamatrix}
            \xrightarrow[R_3\to\,R_3+R_1]{R_2\to\,R_2-2R_1}
            \begin{Lamatrix}{9}{3}
                1 & -2 & 3 & 1 & 0 & 0 \\
                0 & 5 & -5 & -2 & 1 & 0 \\
                0 & -2 & 5 & 1 & 0 & 1 
            \end{Lamatrix} \\
            \xrightarrow{R_2 \to \frac{1}{5}R_2}
            &\begin{Lamatrix}{9}{3}
                1 & -2 & 3 & 1 & 0 & 0 \\
                0 & 1 & -1 & -\frac{2}{5} & \frac{1}{5} & 0 \\
                0 & -2 & 5 & 1 & 0 & 1 
            \end{Lamatrix} 
            \xrightarrow{R_3 \to R_3 + 2R_2}
            \begin{Lamatrix}{9}{3}
                1 & -2 & 3 & 1 & 0 & 0 \\
                0 & 1 & -1 & -\frac{2}{5} & \frac{1}{5} & 0 \\
                0 & 0 & 3 & \frac{1}{5} & \frac{2}{5} & 1 
            \end{Lamatrix} \\
            \xrightarrow[R_3 \to \frac{1}{3}R_3]{R_1\to\,R_1 - R_3} 
            &\begin{Lamatrix}{9}{3}
                1 & -2 & 0 & \frac{4}{5} & -\frac{2}{5} & -1 \\
                0 & 1 & -1 & -\frac{2}{5} & \frac{1}{5} & 0 \\
                0 & 0 & 1 & \frac{1}{15} & \frac{2}{15} & \frac{1}{3} 
            \end{Lamatrix}
            \xrightarrow{R_2 \to R_2 + R_3}
            \begin{Lamatrix}{9}{3}
                1 & -2 & 0 & \frac{4}{5} & -\frac{2}{5} & -1 \\
                0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
                0 & 0 & 1 & \frac{1}{15} & \frac{2}{15} & \frac{1}{3} 
            \end{Lamatrix} \\
            \xrightarrow{R_1 \to R_1 + 2R_2}
            &\begin{Lamatrix}{9}{3}
                1 & 0 & 0 & \frac{2}{15} & \frac{4}{15} & -\frac{1}{3} \\
                0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
                0 & 0 & 1 & \frac{1}{15} & \frac{2}{15} & \frac{1}{3} 
            \end{Lamatrix}
            \Rightarrow
            \boxed{A^{-1} = 
            \begin{pmatrix}
                \frac{2}{15} & \frac{4}{15} & -\frac{1}{3} \\
                -\frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
                \frac{1}{15} & \frac{2}{15} & \frac{1}{3}
            \end{pmatrix}}
        \end{align*}
    \end{solution}
\end{example}