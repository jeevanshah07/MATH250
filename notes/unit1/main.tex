\section{Unit 1}
\subsection{Lecture 1 (1.1-1.2)}

A system of linear equations is when you have more than 1 equation with more than 1 unknown.

\begin{example}{}{}
    Solve the following system:
    \[
        \begin{cases}
            x_1 + 2x_2 &= 5 \\
            2x_1 + x_2 &= 4
        \end{cases}
    \]
    \begin{align*}
        x_1 + 2x_2 = 5 &\Rightarrow x_1 = 5-2x_2 \\
        2x_1 + x_2 = 4 &\Rightarrow 2(5-2x_2) + x_2 = 4 \Rightarrow 10-3x_2=4 \\ 
        &\Rightarrow x_2=2 \\
        &\Rightarrow x_1 = 1
    \end{align*}
\end{example}

Note that the solution to the previous system $(x_1, x_2) = (1, 2)$ also corresponds to the point of intersection of the lines that each equation represents. This then implies that non-parallel lines have a single solution, paralell lines have no solutions, and scalar multiples of the same line have infinitely many solutions.

\begin{example}{Systems with Different Number of Solutions}{}
    \begin{enumerate}
        \item One Solution:
        \[
            \begin{cases}
                x_1 + 2x_2 &= 5 \\
                2x_1 + x_2 &= 4
            \end{cases} 
        \]

        \item No solution:
        \[
            \begin{cases}
                x_1 + 2x_2 &= 5 \\
                2x_1 + 4x_2 &= 6
            \end{cases}
        \]

        \item Infinite Solutions
        \[
            \begin{cases}
                x_1 + 2x_2 &= 5 \\
                2x_1 + 4x_2 &= 10
            \end{cases}
        \]
    \end{enumerate}
\end{example}

\begin{impbox}{Elementary Row Operations}
    Consider the system 
    \[\begin{cases}
        &E_1 \\
        &E_2 \\
        &\vdots \\
        &E_n
    \end{cases}\]
    Where $E_i$ is the $i$-th equation. Then the following operations on the system \textbf{do not change the solution}
    \begin{enumerate}
        \item Changing the order of equations: $E_i \leftrightarrow E_j$ where $i \ne j$ for the $i$-th and $j$-th equation
        \item Scaling equations: $E_i \rightarrow \lambda E_i$ where $\lambda\in\RR\left(\lambda \ne 0\right)$
        \item Combining equations: $E_i \rightarrow E_i + \lambda E_j$ where $i \ne j$ and $\lambda \in \RR \left(\lambda \ne 0\right)$
    \end{enumerate}
\end{impbox}

\begin{example}{}{}
    Solve the following system:
    \[\begin{cases}
        E_1 &= x_1 + 2x_2 = 5 \\
        E_2 &= 2x_1 + x_2 = 4
    \end{cases}\]
    \begin{align*}
        -2E_1 &= -2x_1 - 4x_2 = -10 \\
        E_2 - 2E_1 &= (2x_1 - x_2) - 2x_1 - 4x_2 = 4-10 \\
        &\Rightarrow -3x_2 = -6 \\
        &\Rightarrow x_2 = 2 \\
        &\Rightarrow x_1 = 1
    \end{align*}
\end{example}

\begin{defbox}{Matricies}
    A \textbf{matrix} is a rectangular array of numbers arranged in rows and columns. A matrix $A$ has size $m \times n$ if it has $m$ \textbf{rows} and $n$ \textbf{columns}. A matrix is written as
    \[\underset{m \times n}{A} = \begin{pmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    a_{21} & a_{22} & \dots & a_{2n} \\
    \vdots & & \ddots & \vdots \\
    a_{m1} & a_{m2} & \dots & a_{mn}
    \end{pmatrix}\]
    Where each $a_{ij}$ is called an element of the matrix. $i$ denotes the row and $n$ denotes the column of the element.
\end{defbox}

With this in mind, note that we can represent the previous system as a matrix:
\[\begin{pmatrix}
    1 & 2 & 5 \\
    2 & 1 & 4
\end{pmatrix}\]
where each row represents an equation and each element corresponds to either the coefficent of a variable or the solution. With this system in matrix form we can manipulate the rows as follows:
\[
    \begin{pmatrix}
        1 & 2 & 5 \\
        2 & 1 & 4
    \end{pmatrix} 
    \xrightarrow[R_2 \rightarrow R_2 - 2R_1]{} 
    \begin{pmatrix}
        1 & 2 & 5 \\
        0 & -3 & -6
    \end{pmatrix} 
\]
Since each row corresponds to an equation, if we take a look at the bottom row we can see 
\[ -3x_2 = -6 \Rightarrow x_2 = 2 \Rightarrow x_1 = 1\]
Thus allowing us to reach the same final answer with a lot less hassle. 

\begin{impbox}{General Form of a System of Linear Equations}
    Consider any system of linear equations in the form 
    \[
        \begin{cases}
            a_{11}x_{1} + a_{12}x_{2} + \dots + a_{1n}x_n &= b_1 \\
            a_{21}x_{1} + a_{22}x_{2} + \dots + a_{2n}x_n &= b_2 \\
            \vdots \\
            a_{m1}x_{1} + a_{m2}x_{2} + \dots + a_{mn}x_n &= b_m \\
        \end{cases} 
    \]
    Now, notice we can rewrite this as a matrix:
    \[
        A = \begin{amatrix}{4}
            a_{11} & a_{12} & \dots & a_{1n} & b_1 \\
            a_{21} & a_{22} & \dots & a_{2n} & b_2 \\
            \vdots & & \ddots & \vdots & \vdots \\
            a_{m1} & a_{m2} & \dots & a_{mn} & b_m
        \end{amatrix}
    \]
    Everything to the \textbf{left} of the solid line is refered to as the \textit{coefficent matrix}. $A$ itself is referred to as the \textbf{augmented matrix} for the system.
\end{impbox}

\begin{defbox}{Row Echelon Form}
    Given a matrix $A$, $A_{\text{REF}}$ is an equivalent matrix that satisfies the following properites:
    \begin{enumerate}
        \item All zero rows are below non-zero rows
        \item each next leading element is in the column to the right of the previous leading element (called pivots)
    \end{enumerate}
    Note that the \textbf{leading element} of a row is simply the first non-zero element in that row. 
\end{defbox}

A matrix can also be put in RREF (reduced row echelon form) if it is alreay in REF, each pivot is $1$, and the only non-zero element in the pivot column is the pivot. This would be $A_{\text{RREF}}$. Now, lets try to combine all we've done by applying basic ERO to a simple matrix to put it in REF. 


\begin{example}{Basic REF}{ref}
    Consider the system 
    \[
        \begin{cases}
            x_1 + 2x_2 &= 5 \\
            2x_1 + x_2 = 4
        \end{cases} 
    \]
    It's augmented matrix can be put into REF as follows
    \[
        \begin{amatrix}{1}
            A & B
        \end{amatrix}
        = 
        \begin{amatrix}{2}
            1 & 2 & 5 \\
            2 & 1 & 4
        \end{amatrix}
        \xrightarrow[R_2 \rightarrow R_2 - 2R_1]{} 
        \begin{amatrix}{2}
            1 & 2 & 5 \\
            0 & -3 & -6
        \end{amatrix}
        = 
        \begin{amatrix}{1}
            A & B
        \end{amatrix}_{\text{REF}}
    \]
\end{example}

\begin{example}{Basic RREF}{}
    Consider again the system from ex~(\ref{th:ref}) and notice we can put it into RREF as follows:
    \[
        \begin{amatrix}{1}
            A & B
        \end{amatrix}_{\text{REF}}
        =  
        \begin{amatrix}{2}
            1 & 2 & 5 \\
            0 & -3 & -6
        \end{amatrix}
        \xrightarrow[R_2 \rightarrow -\frac{1}{3}R_2]{} 
        \begin{amatrix}{2}
            1 & 2 & 5 \\
            0 & 1 & 2
        \end{amatrix}
        \xrightarrow[R_1 \rightarrow R_1 - 2R_2]{} 
        \begin{amatrix}{2}
            1 & 0 & 1 \\
            0 & 1 & 2
        \end{amatrix}
        = 
        \begin{amatrix}{1}
            A & B
        \end{amatrix}_{\text{RREF}}
    \]
\end{example}

It's important to note that the RREF for any given matrix is \textit{unique}.

\begin{example}{Solve the System}{}
    \begin{align*}
        \begin{cases}
            x_1 + 2x_2 - x_3 &= 2 \\
            2x_1 + x_2 + x_3 &= 1 \\ 
            x_1 - x_2 + 2x_3 &= -1
        \end{cases}
        = 
        \begin{amatrix}{3}
            1 & 2 & -1 & 2 \\
            2 & 1 & 1 & 1 \\
            1 & -1 & 2 & -1
        \end{amatrix} &\xrightarrow[\substack{R_2 \to R_2 - 2R_1 \\ R_3 \to R_3 - R_1}]{} 
        \begin{amatrix}{3}
            1 & 2 & -1 & 2 \\
            0 & -3 & 3 & -3 \\
            0 & -3 & 3 & -3
        \end{amatrix} \\
        &\xrightarrow[R_3 \to R_3 - R_2]{}
        \begin{amatrix}{3}
            1 & 2 & -1 & 2 \\
            0 & -3 & 3 & -3 \\
            0 & 0 & 0 & 0
        \end{amatrix} \\
    \end{align*}
    Now because there is no pivot in the third column of our matrix, $x_3$ is what's known as a `free variable'. All this means is that $x_3$ can take on any value, we'll notate this by $x_3 = t$ with $t \in \RR$.
    \begin{align*}
        &\Rightarrow\begin{cases}
            x_3 &= t \\
           -3x_2 + 3x_3 &= -3 \\
           x_1 + 2x_2 - x_3 &= 2
        \end{cases} \\
        &\Rightarrow X_{\text{gen}} = \begin{pmatrix}
            -t \\
            1 + t \\
            t
        \end{pmatrix}
    \end{align*}
\end{example}

\subsection{Lecture 2 (1.3-1.4)}

As a reminder from last time, given a system with an augmented matrix 
$\begin{amatrix}{1}
    A & B
\end{amatrix}$
the system will have
\begin{enumerate}
    \item \underline{no} solutions if there is a pivot in the last column of 
    $\begin{amatrix}{1}
        A & B
    \end{amatrix}_{\text{REF}}$
    \item \underline{one} solution if $\begin{amatrix}{1}
        A & B
    \end{amatrix}_{\text{REF}}$ is $m \times n$ with $n$ pivots (and no pivots in the last column)
    \item \underline{infinite} solutions if $\begin{amatrix}{1}
        A & B
    \end{amatrix}_{\text{REF}}$ is $m \times n$ with less than $n$ pivots (and no pivots in the last column)
\end{enumerate}

\begin{defbox}{Vectors}{}
    For a vector $\vec{AB}$, point $A$ is the tail and point $B$ is the head. Two vectors, $\vec{AB}$ and $\vec{CD}$ are equal if, and only if, their magnitude and directions are equal. Vectors in $\RR^{2}$ can be notated as $\big(\begin{smallmatrix} a_1 \\ a_ 2\end{smallmatrix}\big)$ where the point $(a_1, a_2)$ is the head and the tail is (usually) assumed to the origin. Generally, for $\RR^{n}$ we have $\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ which will always have size $n \times 1$.
\end{defbox}

Since we can represent vectors as matricies, we can perform the standard matrix operations on them:

\begin{impbox}{Matrix Operations}{}
    Let $A$ and $B$ be matricies. Then
    \begin{enumerate}
        \item $A + B = C$ where $A$ and $B$ are the same size. This addition is defined as $c_{ij} = a_{ij} + b_{ij}$
        \item $\lambda A = \hat{A}$ where $\hat{a_{ij}} = \lambda a_{ij}$ for $\lambda \in \RR$
        \item $A + (-A) = \hat{0}$ where $\hat{0}$ is the \underline{zero-matrix} which has the same size as $A$ with each element being $0$
        \item Consider matricies $\underset{m \times n}{A}$ and $\underset{n\times 1}{X}$, then $AX$ is defined as
        \[
            \underset{m \times 1}{A} = 
            \begin{pmatrix}
                a_{11}x_{1} & a_{12}x_2 & \dots & a_{1n}x_n \\
                a_{21}x_1 & a_{22}x_2 & \dots & a_{2n}x_n \\
                \vdots & & \ddots & \vdots \\
                a_{m1}x_1 & a_{m2}x_2 & \dots & a_{mn}x_n
            \end{pmatrix}
            \text{ if }
            \underset{m\times n}{A} = 
            \begin{pmatrix}
                a_{11} & a_{12} & \dots & a_{1n} \\
                a_{21} & a_{22} & \dots & a_{2n} \\
                \vdots & & \ddots & \vdots \\
                a_{m1} & a_{m2} & \dots & a_{mn}
            \end{pmatrix}
            \text{ and }
            \underset{n \times 1}{X} = 
            \begin{pmatrix}
                x_1 \\
                x_2 \\
                \vdots 
                \\ 
                x_n
            \end{pmatrix}
        \]
    \end{enumerate}
\end{impbox}

\begin{example}{Basic Matrix Multiplication}{}
    Consider the the matricies 
    \[ 
        \underset{2 \times 3}{A} = 
        \begin{pmatrix}
            1 & -1 & 2 \\
            3 & 4 & 5 
        \end{pmatrix} 
        \text{ and }
        \underset{3 \times 1}{B} = 
        \begin{pmatrix}
            2 \\
            4 \\
            7
        \end{pmatrix}
    \]
    Find $AB$.
    \[ 
        \underset{2 \times 1}{AB} = 
        \begin{pmatrix}
            (1 * 2) + (-1 * 4) + (2 * 7) \\
            (3 * 2) + (4 * 4) + (5 * 7) 
        \end{pmatrix}
        = 
        \begin{pmatrix}
            12 \\
            57
        \end{pmatrix}
    \]
\end{example}

It's important to note that an alternative form of defining matrix multiplication is as follows:
\[
    AX = 
    x_1\begin{pmatrix}
        a_{11} \\ a_{21} \\ \vdots \\ a_{m1}
    \end{pmatrix}
    + x_2\begin{pmatrix}
        a_{12} \\ a_{22} \\ \vdots \\ a_{m2}
    \end{pmatrix}
    + \dots + 
    x_n\begin{pmatrix}
        a_{m1} \\ a_{m2} \\ \vdots \\ a_{mn}
    \end{pmatrix}
\]
This is known as the \textbf{linear combination of vectors}.

\begin{defbox}{Linear Combination of Vectors}{}
    Let $\braces{\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}}$ be a set of vectors from $\RR^{n}$. Then, 
    \[
        c_1\vec{u_1} + c_2\vec{u_2} + \dots + c_k\vec{u_k}, \hspace{0.2in} (c_1, c_2, \dots, c_k \in \RR) 
    \]
    is called the \textbf{linear combination of vectors} $\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}$.
\end{defbox}

We can combine the previous two definitions of matrix multiplication and the linear combination of vectors to get this next fact: if we consider a vector 
\[(\vec{u_k}) = \begin{pmatrix} u_{1k} \\ u_{2k} \\ \vdots \\ u_{nk} \end{pmatrix}\]
then, $c_1\vec{u_1} + c_2\vec{u_2} + \dots + c_k\vec{u_k} = AX$ where
\[
    A = 
    \begin{pmatrix}
        \left(\vec{u_1}\right) & \left(\vec{u_2}\right) & \dots & \left(\vec{u_k}\right)
    \end{pmatrix}
    \text{ and }
    X = \begin{pmatrix}
        c_1 \\ c_2 \\ \vdots \\ c_k
    \end{pmatrix}
\]

This is especially important when working with systems of equations as we can represent systems as linear combinations of vectors. Given any general system we can rewrite it as a linear combination of vectors as such
\[ 
    \left.\begin{cases}
        a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n &= b_1 \\
        a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n &= b_1 \\
        \vdots & \\
        a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n &= b_1 \\
    \end{cases}\right] \Leftrightarrow AX = B
\]
$B$ is said to be a \textit{linear combination of columns of $A$} if, and only if, $A$ and $B$ are \textbf{compatible}. For $A$ and $B$ to be compatible essentially just means that $\begin{amatrix}{1} A & B\end{amatrix}_{\text{REF}}$ has no pivots in the last column.

\begin{defbox}{Span}{}
    $c_1\vec{v_1} + c_2\vec{v_2} + \dots + c_k\vec{v_k} = \text{Span}\braces{\vec{v_1}, \vec{v_2}, \dots, \vec{v_k}}$ where $c_1, c_2, \dots, c_k$ are \underline{all} possible numbers
\end{defbox}

For a single vector $\vec{v}$, $\text{Span}\braces{\vec{v}}$ is simply the set containing all scaled multiples of $\vec{v}$.

\begin{example}{Span of Two Vectors}{}
    Notice that $\text{Span}\braces{\begin{pmatrix} 1 \\ 2\end{pmatrix}, \begin{pmatrix} 3 \\ 4\end{pmatrix}} = \RR^{2}$. This implies that any vector from $\RR^{2}$ can be written as 
    \[ c_1\begin{pmatrix} 1 \\ 2 \end{pmatrix} + c_2\begin{pmatrix} 3 \\ 4\end{pmatrix}\]
    We can prove this fact by considering the augmented matrix for $AX=B$:
    \[ 
        \begin{amatrix}{2}
           1 & 3 & b_1 \\
           2 & 4 & b_2  
        \end{amatrix}
        \xrightarrow[R_2 \to R_2 - 2R_1]{} 
        \begin{amatrix}{2}
            1 & 3 & b_1 \\
            0 & -2 & b_2 - 2b_1
        \end{amatrix}
    \]
    Therefore, since there is no pivote in the last column, this system has a single solution for any vector $B = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$
\end{example}

\begin{example}{}{}
    Given the following vectors $\vec{a_1}, \vec{a_2}, \vec{a_3}$ and $\vec{b}$, determine if $\vec{b}$ is a linear combination of $\vec{a_1}, \vec{a_2}, \vec{a_3}$.
    \[
        \vec{a_1} = \begin{pmatrix} 1 \\ -2 \\ 0 \end{pmatrix} 
        \hspace{0.2cm}
        \vec{a_2} = \begin{pmatrix} 0 \\ 1 \\ 2 \end{pmatrix} 
        \hspace{0.2cm}
        \vec{a_3} = \begin{pmatrix} 5 \\ -6 \\ 8 \end{pmatrix} 
        \hspace{0.2cm}
        \vec{b} = \begin{pmatrix} 2 \\ -1 \\ 6 \end{pmatrix} 
        \hspace{0.2cm}
    \]
    We can start by noticing that $\vec{b} = c_1\vec{a_1} + c_2\vec{a_2} + c_3\vec{a_3} \Leftrightarrow A\begin{pmatrix} c_1 \\ c_2 \\ c_3 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}$ where \\ $A = \begin{pmatrix} \left(\vec{a_1}\right) & \left(\vec{a_2}\right) & \left(\vec{a_3}\right) \end{pmatrix}$. Thus, 
    \[ 
        \begin{amatrix}{3}
            1 & 0 & 5 & 2 \\
            -2 & 1 & -6 & -1 \\
            0 & 2 & 8 & 6 
        \end{amatrix} 
        \xrightarrow[R_2 \to R_2 + 2R_1]{}
        \begin{amatrix}{3}
            1 & 0 & 5 & 2 \\
            0 & 1 & 4 & 3 \\
            0 & 2 & 8 & 6 
        \end{amatrix} 
        \xrightarrow[R_3 \to R_3 - 2R_2]{}
        \begin{amatrix}{3}
            1 & 0 & 5 & 2 \\
            0 & 1 & 4 & 3 \\
            0 & 0 & 0 & 0 
        \end{amatrix} 
    \]
    Therefore, since there is no pivot in the final column, $B$ is a linear combination of $A$.
\end{example}

\subsection{Lecture 3 (1.5)}

\begin{defbox}{Homogenous Systems of Linear Equations}{}
    A \textbf{homogenous system of linear equations} is any system in the form 
    \[Ax = \hat{0}\]
    Thus, any system in the form \[Ax = B\] is called a \textbf{non-homogenous system}. 
\end{defbox}

For a non-homogenous system we can follow the steps
\[ Ax = B \Rightarrow \begin{amatrix}{1} A & B \end{amatrix} \xrightarrow[\text{ERO}]{} \begin{amatrix}{1} A & B \end{amatrix}_{\text{REF}} \Rightarrow \text{backwards substitution into the equations}\]
For each non-homogenous system:
\begin{enumerate}
    \item if the augmented matrix has a pivote in the last column, it has no solutions
    \item if $A$ is $m \times n$ and the augmented matrix has less than $n$ pivots, then $Ax=B$ has an infinite amount of solutions
    \item if $A$ is $m \times n$ and the augmented matrix has exactly $n$ pivots then $Ax=B$ has \underline{one} solution
\end{enumerate}
For a homogenous system we can follow similar steps:
\[Ax = \hat{0} \Rightarrow \begin{amatrix}{1} A & 0 \end{amatrix}\xrightarrow[\text{ERO}]{} A_{\text{REF}} \Rightarrow \text{backwards substitution to the equations}\]
Note that since the solution matrix is simply just the zero matrix, we dont need to consider it in our augmented matrix - thats why you see $A_\text{REF}$. Unlike non-homogenous systems there are only two options for solutions:
\begin{enumerate}
    \item $x=\hat{0}$ is \underline{always} a solution, this is called the \textit{trivial solution}. If $A_\text{REF}$ is $m \times n$ large and has $n$ pivots, then $x=\hat{0}$ is the \textbf{only} solution
    \item if $A_{\text{REF}}$ is $m \times n$ and has less than $n$ pivots, then $Ax=\hat{0}$ has an \textbf{infinite} amount of solutions
\end{enumerate}

\begin{example}{Solve the following System}{}
    Solve the below system
    \[\begin{cases}
        x_1 + 2x_2 - 3x_3 + x_4 &= 0 \\
        2x_2 - x_2 + x_3 - 2x_4 &= 0 \\
        3x_1 + x_2 - x_3 + x_4 &= 0
    \end{cases}\]
    We can start by putting this in the form $Ax=\hat{0}$ where 
    \[\underset{3 \times 4}{A} = \begin{pmatrix}
        1 & 2 & -3 & -1 \\
        2 & -1 & 1 & -2 \\
        3 & 1 & -1 & 1
    \end{pmatrix}, \hspace{0.2cm}
    \underset{4 \times 1}{X} = \begin{pmatrix}
        x_1 \\ x_2 \\ x_3 \\ x_4
    \end{pmatrix}\]
    Next, we need to put $A$ into REF: 
    \[
        \underset{3 \times 4}A \xrightarrow[\substack{R_2 \to R_2 - 2R_1 \\ R_3 \to R_3 - 3R_1}]{} 
        \begin{pmatrix}
            1 & 2 & -3 & -1 \\
            0 & -5 & 7 & -4 \\
            0 & -5 & 8 & -2
        \end{pmatrix} 
        \xrightarrow[R_3 \to R_3 - R_2]{}
        \begin{pmatrix}
            1 & 2 & -3 & -1 \\
            0 & -5 & 7 & -4 \\
            0 & 0 & 1 & 2
        \end{pmatrix} 
        = A_{\text{REF}}
    \]
    Since there is no pivot in the fourth column, $x_4$ must be a free variable. We can rename it as $x_4=t$ for $t \in \RR$. Thus
    \[
        \left.\begin{cases}
            x_3 + 2x_4 &= 0 \Rightarrow x_3 = -2t \\
            -5x_2 + 7x_3 - 4x_3 &= 0 \Rightarrow x_2 = -\frac{18}{5}t \\
            x_1 + 2x_2 - 3x_3 + x_4 &0 = 0 \Rightarrow x_1 = \frac{1}{5}t 
        \end{cases}\right]
        \Rightarrow
        X = \begin{pmatrix} \frac{1}{5}t \\ -\frac{18}{5}t \\ -2t \\ t \end{pmatrix}
        \Leftrightarrow 
        \begin{pmatrix}
            1 \\ -18 \\ -10 \\ 5
        \end{pmatrix}t
    \]
    Notice that since $t$ is any arbitary value we can make $t = 5t$ in order to eliminate the fractions. As well, the second matrix with $t$ on the outside is known as the \textit{parametric form} of the solution.
\end{example}

\begin{thm}{}{}
    The general solution of a consisten non-homogenous system $Ax=B$ has the form $X_{\text{gen}} = X_{\text{genhom}} + X_{\text{p}}$ where $X_{\text{genhom}}$ is the general solution of the corresponding homogenous system $Ax=\hat{0}$ and $X_p$ is any particular solution of $Ax=B$.
    \begin{proof}
       Let $X_1$ and $X_2$ be any two solutions of $Ax=B$. That is, 
       \[\left.\begin{cases}
            AX_1 &= B \\
            AX_2 &= B
       \end{cases}\right]
       \Rightarrow AX_1 - AX_2 = 0 \Leftrightarrow A(X_1-X_2) = 0\] 
       Therefore, $X_1 - X_2$ is a solution of the homogenous system. With some clever `renaming' we can see:
       \[X_1 - X_2 = X_{\text{hom}} \Rightarrow X_1 = X_{\text{hom}} + X_2 \Rightarrow X_{\text{gen}} = X_{\text{genhom}} + X_{p}\]
    \end{proof}
\end{thm}

\begin{example}{Solve the System}{}
    \[\begin{cases}
        x_1 + 2x_2 - x_3 &= 4 \\
        2x_1 + x_2 - 2x_3 &= 2 \\
        x_1 + x_2 - x_3 &= 2
    \end{cases}\]
    As always, we start by forming the augmented matrix and putting it into REF:
    \[\begin{amatrix}{3}
        1 & 2 & -1 & -4 \\
        2 & 1 & -2 & 2 \\
        1 & 1 & -1 & 2
    \end{amatrix}
    \xrightarrow[\substack{R_2 \to R_2 - 2R_1 \\ R_3 \to R_3 - R_2}]{}
    \begin{amatrix}{3}
        1 & 2 & -1 & 4 \\
        0 & -3 & 0 & -6 \\
        0 & -1 & 0 & -2
    \end{amatrix}
    \xrightarrow[\substack{R_2 \to -\frac{1}{3}R_2 \\ R_3 \to R_3 - \frac{1}{3}R_2}]{} 
    \begin{amatrix}{3}
        1 & 2 & -1 & 4 \\
        0 & 1 & 0 & 2 \\
        0 & 0 & 0 & 0
    \end{amatrix}
    \] 
    \[
    \xrightarrow[R_1 \to R_1 - 2R_2]{}
    \begin{amatrix}{3}
        1 & 0 & -1 & 0 \\
        0 & 1 & 0 & 2 \\
        0 & 0 & 0 & 0
    \end{amatrix}
    \]
    Now, since there is no pivot in the third column, $x_3$ must be a free variable, so we have $x_3=t$ for $t \in \RR$. Using some very basic backwards substitution we can see that $x_2 = 2$ and $x_1 = x_3 = t$. As well, notice that the final form of the augmented matrix is actually in RREF and not REF, because of how easy it was to put the matrix in RREF from REF, there was almost no reason \textbf{not} to put it into RREF. We can now write our solution vector,
    \[X_{\text{gen}} = \begin{pmatrix} t \\ 2 \\ t\end{pmatrix} = \underbrace{\begin{pmatrix}
    0 \\ 2 \\ 0
    \end{pmatrix}}_{X_p} + t\underbrace{\begin{pmatrix}
    1 \\ 0 \\ 1
    \end{pmatrix}}_{X_{\text{genhom}}}\]
\end{example}

\begin{example}{Find the General Homogenous Solution}{}
    Just to prove the point, lets find the general homogenous solution for the above system, $Ax = \hat{0}$. 
    \[A = \begin{pmatrix}
        1 & 2 & -1 \\
        2 & 1 & -2 \\
        1 & 1 & -1
    \end{pmatrix}
    \xrightarrow[\substack{R_2 \to R_2 - 2R_2 \\ R_3 \to R_3 - R_1}]{} 
    \begin{pmatrix}
        1 & 0 & -1 \\
        0 & 1 & 0 \\
        0 & 0 & 0
    \end{pmatrix} = A_{\text{RREF}} 
    \]
    With backwards substitution we can easily see that $x_3=t$ for $t \in \RR$, $x_2=0$, and $x_1 = t$. So, 
    \[X_{\text{genhom}} = \begin{pmatrix}
        t \\ 0 \\ t
    \end{pmatrix} = t\begin{pmatrix}
        1 \\ 0 \\ 1
    \end{pmatrix}\]
    Which, as expected, is exactly the same vector we saw in the previous example.
\end{example}

\begin{example}{Solve the System}{}
    \[\begin{cases}
        x_1 - 2x_2 + x_3 + x_4 &= 0 \\
        2x_1 + x_2 - 2x_3 + x_4 &= 0 \\
        3x_1 - x_2 - x_3 &= 0 
    \end{cases}\]
    Once again, we must put $A$ into REF:
    \[A \xrightarrow[\substack{R_2 \to R_2 - 2R_1 \\ R_3 \to R_3 - 3R_1}]{} \begin{pmatrix}
        1 & -2 & 1 & -1 \\
        0 & 5 & -4 & 3 \\
        0 & 5 & -4 & 3
    \end{pmatrix}
    \xrightarrow[R_3 \to R_3 - R_2]{} 
    \begin{pmatrix}
        1 & -2 & 1 & 1 \\
        0 & 5 & -4 & 3 \\
        0 & 0 & 0 & 0 
    \end{pmatrix}
    \]
    Now, because there are no pivots in the third and fourth column, $x_3$ and $x_4$ are free variables which makes $x_1$ and $x_2$ our basic variables $\therefore x_3 = t, x_4 = s$ for $s, t \in \RR$
    \[
        \begin{cases}
            5x_2 - 4x_3 + 3x_4 &= 0 \Rightarrow x_2 = \frac{4}{5}x_3 - \frac{3}{5}x_4 = \frac{4}{5}t - \frac{3}{5}s \\
            x_1 - 2x_2 + x_3 - x_4 &= 0 \Rightarrow x_1 = 2x_2 - x_3 + x_4 = 2(\frac{4}{5}t - \frac{3}{5}s) - t + s
        \end{cases} 
    \]
    Before continuing, it's smart to make $t = 5t$ and $s=5s$ in order to eliminate the fractions. Then we have:
    \[X_{\text{gen}} = \begin{pmatrix}
        3t - s \\ 4t - 3s \\ 5t \\ 5s
    \end{pmatrix} = t\begin{pmatrix}
        3 \\ 4 \\ 5 \\ 0 
    \end{pmatrix} + s\begin{pmatrix}
        -1 \\ -3 \\ 0 \\ 0 
    \end{pmatrix}\]
\end{example}

\subsection{Lecture 4: 1.3}
\begin{defbox}{Rank of a Matrix}{}
    Let $\underset{m \times n}{A}$ be a matrix. Then $\text{rank}(A)$ is the number of pivots in $A_{\text{REF}}$. $\text{rank}(A)$ is equivalent to the number of non-zero rows in $A_{\text{REF}}$. 
\end{defbox}

It follows from the definition of $\text{rank}(A)$ that the number of free variables in a system is $n - \text{rank}(A)$. This quanitity is known as $\text{null}(A)$.

If $\underset{n \times n}{A}$ is a matrix such that $\text{rank}(A) = n$, then $A_{\text{REF}} = I$ where $I$ is the identity matrix.

\begin{defbox}{Identity Matrix}{}
    The identity matrix, $I$, is a square matrix which is automatically in RREF and its only non-zero elements are the pivots. For example
    \[I_2 = \begin{pmatrix}
        1 & 0 \\
        0 & 1
    \end{pmatrix}
    \hspace{0.2cm}
    \text{and}
    \hspace{0.2cm}
    I_3 = \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}\]
\end{defbox}

Another important fact about the rank of matrix is that it can only be as great as the minimimum of $n$ and $m$ for a matrix of size $m \times n$. Formally, 
\[ \text{rank}\left(\underset{m \times n}{A}\right) \leq \min\braces{m, n}\]
where $\min\braces{x_1, x_2, \dots, x_n}$ returns the smallest value out of $x_1, x_2, \dots, x_n$.

\begin{defbox}{Span of Vectors from $\RR^{n}$}{}
    The span of a set of vectors from $\RR^{n}$, $\text{Span}\braces{\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}}$ is the set of all possible linear combinations of $\vec{u_1}, \vec{u_2}, \dots, \vec{u_k}$:
    \[AC = c_1\vec{u_1} + c_2\vec{u_2} + \dots + c_k\vec{u_k}, \hspace{0.4cm}\left(c_1, c_2, \dots, c_k \in \RR \right)\]
    where 
    \[
        A = \begin{pmatrix}
            \left(\vec{u_1}\right) & \left(\vec{u_2}\right) & \dots & \left(\vec{u_k}\right) 
        \end{pmatrix}
        \text{ and } 
        C = \begin{pmatrix}
            c_1 \\ c_2 \\ \vdots \\ c_k 
        \end{pmatrix}
    \]
\end{defbox}

The span of the zero vector, is also the zero vector: $\text{Span}\braces{\vec{0}} = \vec{0}$

\begin{example}{Span of a Single Vector}{}
    Consider a vector $u_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$. Then $\Span{\vec{u_1}} = c\vec{u_1}$ for any $c \in \RR$. Geometrically, this is the set of vectors in the same, or opposite, direction as $\vec{u_1}$ with a scaled magnitude. When put together, $\Span{\vec{u_1}}$ gives a line in $\RR^{2}$.
\end{example}

\begin{example}{Span of Two Vectors}{}
    Consider $\vec{u_1} = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$ and $\vec{u_2} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$. Then, $\Span{\vec{u_1}, \vec{u_2}} = \RR^{2}$ because any vector from $\RR^2$ can be writtin in the form 
    \[
        c_1\vec{u_1} + c_2\vec{u_2} = c_1\begin{pmatrix} 1 \\ 2 \end{pmatrix} + c_2\begin{pmatrix} 1 \\ 2 \end{pmatrix}
    \]
    \begin{proof}
        Consider the system 
        \[
            c_1\begin{pmatrix} 1 \\ 2 \end{pmatrix} + c_2\begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} 
            \Leftrightarrow 
            \begin{pmatrix}
                1 & 1 \\
                2 & 1
            \end{pmatrix}
            \begin{pmatrix}
                c_1 \\ c_2
            \end{pmatrix}
            = 
            \begin{pmatrix}
                b_1 \\ b_2
            \end{pmatrix}
            \Leftrightarrow 
            A \begin{pmatrix}
                c_1 \\ c_2 
            \end{pmatrix}
            = 
            \begin{pmatrix}
                b_1 \\ b_2
            \end{pmatrix}, 
            \hspace{0.2cm}
            \underset{2 \times 2}{A} = \begin{pmatrix}
                1 & 1 \\
                2 & 1
            \end{pmatrix}
        \]
        Then, the augmented matrix is 
        \[
            \begin{amatrix}{2}
                1 & 1 & b_1 \\
                2 & 1 & b_2
            \end{amatrix}
            \xrightarrow[R_2 \to R_2 - 2R_1]{} 
            \begin{amatrix}{2}
                1 & 1 & b_1 \\
                0 & -1 & b_2 - 2b_1
            \end{amatrix}
            = 
            \begin{amatrix}{1}
                A & B
            \end{amatrix}_{\text{REF}}
        \]
        $\therefore$ since the augmented matrix in REF has no pivots in the final column, the system is compatible and contains a solution for all vectors $\begin{pmatrix} b_1 \\ b_2\end{pmatrix} \in \RR$.
    \end{proof}
\end{example}

Note as well, that we could've written the final justification using the rank of augmented matrix as reasoning: `$\therefore \text{rank}(A) = 2$, so the system is compatible\dots'. As well, note that since $A$ is a square matrix and $\text{rank}(A) = 2$, that also implies that $A_{\text{RREF}} = I_2$ (it should be pretty easy to put $A$ into RREF and see this for yourself).

\begin{example}{Span of Three Vectors}{}
    Consider three vectors
    \[
        \vec{u_1} = \begin{pmatrix}
            1 \\ 2
        \end{pmatrix},
        \hspace{0.2cm}
        \vec{u_2} = \begin{pmatrix}
            2 \\ 4
        \end{pmatrix}, 
        \hspace{0.2cm}
        \vec{u_3} = \begin{pmatrix}
            3 \\ 6
        \end{pmatrix}
    \]
    and consider the span of $\vec{u_1}, \vec{u_2}$ and $\vec{u_3}$, $\Span{\vec{u_1}, \vec{u_2}, \vec{u_3}}$. Now notice that $u_2 = 2u_1$ and $u_3 = 3u_3$. This implies that $\Span{\vec{u_1}, \vec{u_2}, \vec{u_3}} = \Span{\vec{u_1}}$. $u_2$ and $u_3$ are considered `redudant' in the span.
    \begin{proof}
        Consider the matrix representing the vectors of the generation set:
        \[
            \begin{pmatrix}
                1 & 2 & 3 \\
                2 & 4 & 6
            \end{pmatrix}
            \xrightarrow[R_2 \to R_2 - 2R_1]{}
            \begin{pmatrix}
                1 & 2 & 3 \\
                0 & 0 & 0
            \end{pmatrix}
        \]
        $\therefore$ no pivots in the second and third columns implies that the second and third vectors are redudant. 
    \end{proof}
\end{example}


\begin{example}{Span of Three Vectors (again)}
    Now consider $\vec{u_1}, \vec{u_2}, \vec{u_3}$ such that 
    \[
        \vec{u_1} = \begin{pmatrix}
            1 \\ 3
        \end{pmatrix}, 
        \hspace{0.2cm}
        \vec{u_2} = \begin{pmatrix}
            2 \\ 1
        \end{pmatrix}, 
        \hspace{0.2cm}
        \vec{u_3} = \begin{pmatrix}
            4 \\ 6
        \end{pmatrix}
    \]
    Notice that $\Span{\vec{u_1}, \vec{u_2}, \vec{u_3}}$ \textbf{must} be reducible since the corresponding matrix has dimensions $2 \times 3 \Rightarrow \text{rank}{A} \leq \min{2, 3} = 2 \therefore$ (at least) one vector is redudant. We can figure out which one as follows 
    \[
        \underset{2 \times 3}{A} = \begin{pmatrix}
            1 & 2 & 4 \\
            3 & 1 & 6 
        \end{pmatrix}
        \xrightarrow[R_2 \to R_2 - 3R_1]{}
        \begin{pmatrix}
            1 & 2 & 4 \\
            0 & -5 & -6
        \end{pmatrix}
    \]
    Thus, no pivot in the third column implies that the third vector is redudant:
    \[\therefore \Span{\vec{u_1}, \vec{u_2}, \vec{u_3}} = \Span{\vec{u_1}, \vec{u_2}}\]
\end{example}

\begin{example}{Proving a Vector is in a Span}{}
    Show that $\begin{pmatrix} 4 \\ 6 \end{pmatrix} \in \Span{\begin{pmatrix} 1 \\ 3 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix}}$. We can start with the system 
    \[
        c_1\begin{pmatrix} 1 \\ 3 \end{pmatrix} + c_2\begin{pmatrix} 2 \\ 1\end{pmatrix} = \begin{pmatrix} 4 \\ 6 \end{pmatrix}
        \Rightarrow 
        \begin{amatrix}{2}
            1 & 2 & 4 \\
            3 & 1 & 6
        \end{amatrix}
        \xrightarrow[R_2 \to R_2 - 3R_1]{}
        \begin{amatrix}{2}
            1 & 2 & 4 \\
            0 & -5 & -6
        \end{amatrix}
    \]
    This gives 
    \[
        \left.\begin{cases}
            -5c_2 &= 6 \\
            c_1 + 2c_2 &= 4
        \end{cases}\right]
        \Rightarrow 
        c_2 = \frac{6}{5} \text{ and } c_2 = \frac{8}{5}
    \]
    Note that we knew that $\begin{pmatrix} 4 \\ 5 \end{pmatrix} \in \Span{\begin{pmatrix} 1 \\ 3 \end{pmatrix}, \begin{pmatrix} 2 \\ 1 \end{pmatrix}}$ since the system was compatible, we just found the values of $c_1$ and $c_2$ such that 
    \[
        c_1\begin{pmatrix} 1 \\ 3 \end{pmatrix} + c_2\begin{pmatrix} 2 \\ 1\end{pmatrix} = \begin{pmatrix} 4 \\ 6 \end{pmatrix}
    \]
\end{example}

\begin{example}{Geometric Interpretation of $\RR^{3}$ Span}{}
    Consider vectors $\vec{u_1}, \vec{u_2}$ and give a geometric interpretation of their span, $\Span{\vec{u_1}, \vec{u_2}}$
    \[
        \vec{u_1} = \begin{pmatrix} 8 \\ 2 \\ -6 \end{pmatrix}, 
        \hspace{0.2cm} 
        \vec{u_2} = \begin{pmatrix} 12 \\ 3 \\ -9 \end{pmatrix}
    \]
    Start by noticing that $\vec{u_2} = \frac{3}{2}\vec{u_1} \therefore u_1 \parallel u_2 \Rightarrow \Span{\vec{u_1}, \vec{u_2}}$ must be a line in $\RR^{3}$ along $\vec{u_1}$.
\end{example}

\begin{example}{Geometric Interpretation of $\RR^{3}$ Span (again)}
    Consider vectors $\vec{u_1}, \vec{u_2}$ and give a geometric interpretation of their span, $\Span{\vec{u_1}, \vec{u_2}}$
    \[
        \vec{u_1} = \begin{pmatrix} 8 \\ 2 \\ -6 \end{pmatrix}, 
        \hspace{0.2cm} 
        \vec{u_2} = \begin{pmatrix} 12 \\ 3 \\ 9 \end{pmatrix}
    \]
    Since $\vec{u_1} \nparallel \vec{u_2}, \Span{\vec{u_1}, \vec{u_2}}$ makes a plane in $\RR^{3}$ (spanned by $\vec{u_1}, \vec{u_2}$)
\end{example}

\begin{example}{}{}
    Let $A$ and $b$ be matricies such that 
    \[
        \underset{3 \times 3}{A} = \begin{pmatrix}
            1 & 0 & -4 \\
            0 & 3 & -2 \\
            -2 & 6 & 3
        \end{pmatrix}
        = 
        \begin{pmatrix}
            \left(\vec{u_1}\right) & \left(\vec{u_2}\right) & \left(\vec{u_3}\right)
        \end{pmatrix}
        \text{ and }
        b = \begin{pmatrix}
            4 \\ 1 \\ -4
        \end{pmatrix}
    \]
    Now consider $\mathcal{W} = \Span{\vec{u_1}, \vec{u_2}, \vec{u_3}}$.
    \begin{parts}
        \part Is $b \in \mathcal{W}$?
        \begin{solution}
            Start by assuming that $c \in \mathcal{W} \therefore c_1\vec{u_1} + c_2\vec{u_2} + c_3\vec{u_3} = b \Leftrightarrow$
            \[
                \begin{amatrix}{3}
                    1 & 0 & -4 & 4 \\
                    0 & 3 & -2 & 1 \\
                    -2 & 6 & 3 & -4
                \end{amatrix} 
                \xrightarrow[R_3 \to R_3 + 2R_1]{}
                \begin{amatrix}{3}
                    1 & 0 & -4 & 4 \\
                    0 & 3 & -2 & 1 \\
                    0 & 6 & -5 & 4
                \end{amatrix} 
                \xrightarrow[R_3 \to R_3 - 2R_2]{}
                \begin{amatrix}{3}
                    1 & 0 & -4 & 4 \\
                    0 & 3 & -2 & 1 \\
                    0 & 0 & -1 & 2
                \end{amatrix} 
            \]
            Thus, $\text{rank}{\left(A_{\text{REF}}\right)} = 3 \therefore$ the system is compatible and $b \in \mathcal{W}$
        \end{solution}

        \part Is $\Span{\vec{u_1}, \vec{u_2}, \vec{u_3}}$ reducible?
        \begin{solution}
            \[
                \underset{3 \times 3}{A} = \begin{pmatrix}
                    1 & 0 & -4 \\
                    0 & 3 & -2 \\
                    -2 & 6 & 3 
                \end{pmatrix}
                \xrightarrow[R_3 \to R_3 + 2R_1]{}
                \begin{pmatrix}
                    1 & 0 & -4 \\
                    0 & 3 & -2 \\
                    0 & 6 & -5 
                \end{pmatrix} 
                \xrightarrow[R_3 \to R_3 - 2R_2]{}
                \begin{pmatrix}
                    1 & 0 & -4 \\
                    0 & 3 & -2 \\
                    0 & 0 & -1 
                \end{pmatrix} 
            \]
            $\text{rank}\left(A_\text{REF}\right) = 3 \therefore$ the span is irreducible.
        \end{solution}

        \part Is $\Span{\vec{u_1}, \vec{u_2}, \vec{u_3}} = \Span{\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}}$
        \begin{solution}
            Yes, they're both $\RR^{3}$
        \end{solution}
    \end{parts}

\end{example}